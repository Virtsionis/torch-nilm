{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NilmTorch - Map Dataset vs Iterable Dataset\n",
    "\n",
    "- Auothors: khalid OUBLAL, Nikolaos Virtsionis, Christoforos Nalmpantis\n",
    "\n",
    "There are two main types of dataset objects in PyTorch: Dataset and IterableDataset. The choice between these two types depends on the size of the dataset. In general, an IterableDataset is suitable for large datasets, typically in the range of hundreds of gigabytes, due to its lazy behavior and faster processing. On the other hand, a Dataset is more appropriate for smaller datasets.\n",
    "\n",
    "One key difference between the two is in how the data is accessed. With a regular Dataset, you can access specific rows using indexing, such as hdf5[i]. This random access capability is often referred to as \"map-style\" access. In contrast, IterableDataset provides a ```streaming-like``` access where the data is accessed in a sequential manner.\n",
    "\n",
    "To illustrate, consider the example of downloading the ImageNet-1k dataset. Using a regular Dataset, you can download the dataset and access any specific row as needed. However, when using an IterableDataset, the data is streamed sequentially, allowing for efficient processing without the need to load the entire dataset into memory at once.\n",
    "\n",
    "Overall, the choice between Dataset and IterableDataset depends on the size and nature of the dataset, with IterableDataset being more suitable for large datasets requiring streaming-like access, and Dataset being a versatile option for smaller datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "from constants.constants import *\n",
    "import h5py\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) NilmTorch_iterableDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NilmTorch_iterableDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self,type_files=\"hdf5\",\n",
    "                 data_files = None,\n",
    "                 sequence_length = 256,\n",
    "                 stride = 1,\n",
    "                 FILE_HDF5_list = ['ID_101'], \n",
    "                 streaming = True):\n",
    "        \"\"\"\n",
    "        Open Open file streaming, without any loading to memory\n",
    "        Not like map dataset, which map the whole data at one and then using it\n",
    "        \"\"\"\n",
    "        # Multi correlation le tau d'information\n",
    "        self.sequence_length = sequence_length\n",
    "        self.stride = stride\n",
    "        self.FILE_HDF5_list = FILE_HDF5_list\n",
    "        if type_files in [\"hdf5\", \"HDF5\"]:\n",
    "            self.FILE_HDF5 = h5py.File(data_files, 'r+')\n",
    "            \n",
    "        else:\n",
    "            raise print(f'Not supported {type_files}')\n",
    "\n",
    "    def __iter__(self):\n",
    "        for item in self.FILE_HDF5_list:\n",
    "            # do any preprocessing or data augmentation here\n",
    "            resolution = self.FILE_HDF5[item].attrs['RESOLUTION']\n",
    "            LEN = self.FILE_HDF5[item].attrs['LEN']\n",
    "            X = np.array(self.FILE_HDF5[item]['data'][:])\n",
    "            ground = np.array(self.FILE_HDF5[item]['label'][:])\n",
    "            \n",
    "            for i in range(0, len(X)-self.sequence_length+1, self.stride):\n",
    "                    sequence = self.padding_seqs(X[i:i+self.sequence_length]) #\n",
    "                    yield sequence.transpose(1,0)\n",
    "            \n",
    "    def padding_seqs(self, in_array):\n",
    "        if len(in_array) == self.sequence_length:\n",
    "            return in_array\n",
    "        try:\n",
    "            out_array = np.zeros((self.sequence_length, in_array.shape[1]))\n",
    "        except:\n",
    "            out_array = np.zeros(self.sequence_length)\n",
    "        out_array[:len(in_array)] = in_array\n",
    "        return out_array\n",
    "    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) ShuffleDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffleDataset(torch.utils.data.IterableDataset):\n",
    "  def __init__(self, dataset, buffer_size):\n",
    "    super().__init__()\n",
    "    self.dataset = dataset\n",
    "    self.buffer_size = buffer_size\n",
    "\n",
    "  def __iter__(self):\n",
    "    shufbuf = []\n",
    "    try:\n",
    "      dataset_iter = iter(self.dataset)\n",
    "      for i in range(self.buffer_size):\n",
    "        shufbuf.append(next(dataset_iter))\n",
    "    except:\n",
    "      self.buffer_size = len(shufbuf)\n",
    "\n",
    "    try:\n",
    "      while True:\n",
    "        try:\n",
    "          item = next(dataset_iter)\n",
    "          evict_idx = random.randint(0, self.buffer_size - 1)\n",
    "          yield shufbuf[evict_idx]\n",
    "          shufbuf[evict_idx] = item\n",
    "        except StopIteration:\n",
    "          break\n",
    "      while len(shufbuf) > 0:\n",
    "        yield shufbuf.pop()\n",
    "    except GeneratorExit:\n",
    "      pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-nilm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f26f6606d89fcac7e002dedab18d89dd8b79f5e69a5ac0f81334e43554cd801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
