{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NilmTorch - Map Dataset vs Iterable Dataset\n",
    "\n",
    "- Auothors: khalid OUBLAL, Nikolaos Virtsionis, Christoforos Nalmpantis\n",
    "\n",
    "There are two main types of dataset objects in PyTorch: Dataset and IterableDataset. The choice between these two types depends on the size of the dataset. In general, an IterableDataset is suitable for large datasets, typically in the range of hundreds of gigabytes, due to its lazy behavior and faster processing. On the other hand, a Dataset is more appropriate for smaller datasets.\n",
    "\n",
    "One key difference between the two is in how the data is accessed. With a regular Dataset, you can access specific rows using indexing, such as hdf5[i]. This random access capability is often referred to as \"map-style\" access. In contrast, IterableDataset provides a ```streaming-like``` access where the data is accessed in a sequential manner.\n",
    "\n",
    "To illustrate, consider the example of downloading the ImageNet-1k dataset. Using a regular Dataset, you can download the dataset and access any specific row as needed. However, when using an IterableDataset, the data is streamed sequentially, allowing for efficient processing without the need to load the entire dataset into memory at once.\n",
    "\n",
    "Overall, the choice between Dataset and IterableDataset depends on the size and nature of the dataset, with IterableDataset being more suitable for large datasets requiring streaming-like access, and Dataset being a versatile option for smaller datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "from constants.constants import *\n",
    "import h5py\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) NilmTorch_iterableDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NilmTorch_iterableDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self,type_files=\"hdf5\",\n",
    "                 data_files = None,\n",
    "                 sequence_length = 256,\n",
    "                 stride = 1,\n",
    "                 FILE_HDF5_list = ['ID_101'], \n",
    "                 streaming = True):\n",
    "        \"\"\"\n",
    "        Open Open file streaming, without any loading to memory\n",
    "        Not like map dataset, which map the whole data at one and then using it\n",
    "        \"\"\"\n",
    "        # Multi correlation le tau d'information\n",
    "        self.sequence_length = sequence_length\n",
    "        self.stride = stride\n",
    "        self.FILE_HDF5_list = FILE_HDF5_list\n",
    "        if type_files in [\"hdf5\", \"HDF5\"]:\n",
    "            self.FILE_HDF5 = h5py.File(data_files, 'r+')\n",
    "            \n",
    "        else:\n",
    "            raise print(f'Not supported {type_files}')\n",
    "\n",
    "    def __iter__(self):\n",
    "        for item in self.FILE_HDF5_list:\n",
    "            # do any preprocessing or data augmentation here\n",
    "            resolution = self.FILE_HDF5[item].attrs['RESOLUTION']\n",
    "            LEN = self.FILE_HDF5[item].attrs['LEN']\n",
    "            X = np.array(self.FILE_HDF5[item]['data'][:])\n",
    "            ground = np.array(self.FILE_HDF5[item]['label'][:])\n",
    "            \n",
    "            for i in range(0, len(X)-self.sequence_length+1, self.stride):\n",
    "                    sequence = self.padding_seqs(X[i:i+self.sequence_length]) #\n",
    "                    yield sequence.transpose(1,0)\n",
    "            \n",
    "    def padding_seqs(self, in_array):\n",
    "        if len(in_array) == self.sequence_length:\n",
    "            return in_array\n",
    "        try:\n",
    "            out_array = np.zeros((self.sequence_length, in_array.shape[1]))\n",
    "        except:\n",
    "            out_array = np.zeros(self.sequence_length)\n",
    "        out_array[:len(in_array)] = in_array\n",
    "        return out_array\n",
    "    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) ShuffleDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffleDataset(torch.utils.data.IterableDataset):\n",
    "  def __init__(self, dataset, buffer_size):\n",
    "    super().__init__()\n",
    "    self.dataset = dataset\n",
    "    self.buffer_size = buffer_size\n",
    "\n",
    "  def __iter__(self):\n",
    "    shufbuf = []\n",
    "    try:\n",
    "      dataset_iter = iter(self.dataset)\n",
    "      for i in range(self.buffer_size):\n",
    "        shufbuf.append(next(dataset_iter))\n",
    "    except:\n",
    "      self.buffer_size = len(shufbuf)\n",
    "\n",
    "    try:\n",
    "      while True:\n",
    "        try:\n",
    "          item = next(dataset_iter)\n",
    "          evict_idx = random.randint(0, self.buffer_size - 1)\n",
    "          yield shufbuf[evict_idx]\n",
    "          shufbuf[evict_idx] = item\n",
    "        except StopIteration:\n",
    "          break\n",
    "      while len(shufbuf) > 0:\n",
    "        yield shufbuf.pop()\n",
    "    except GeneratorExit:\n",
    "      pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences in Speed:\n",
    "\n",
    "There is a distinction in speed between regular Dataset objects and IterableDataset in PyTorch. The choice between these two types depends on the size of the dataset. Generally, IterableDataset is more suitable for large datasets, particularly those in the order of hundreds of gigabytes, due to its lazy behavior and faster processing. On the other hand, Dataset is recommended for smaller datasets.\n",
    "\n",
    "When using a regular Dataset, you can access specific rows using indexing, such as htdf5[0], which enables random access. This type of dataset is commonly referred to as a \"map-style\" dataset. For example, you can download and access any row from the dataset.\n",
    "\n",
    "Regular Dataset (Map) objects leverage Arrow, which facilitates efficient random access by memory mapping and in-memory formatting. This approach minimizes costly system calls and deserialization when reading data from disk. Additionally, iterating over the dataset using a for loop benefits from accessing contiguous Arrow record batches, further enhancing speed.\n",
    "\n",
    "However, if your Dataset includes an indices mapping, such as after applying shuffling with Dataset.shuffle(), the speed may decrease by up to 10 times. This reduction in performance is due to the additional step required to obtain the row index using the indices mapping, as well as the loss of reading data in contiguous chunks. To restore the original speed, you would need to rewrite the dataset on disk using Dataset.flatten_indices(), removing the indices mapping. Keep in mind that this process can be time-consuming, especially for larger datasets.\n",
    "\n",
    "In summary, regular Dataset objects excel at random access and efficient data loading, but the presence of an indices mapping can significantly impact speed. Consider the dataset structure and the need for indices mapping when optimizing performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-nilm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f26f6606d89fcac7e002dedab18d89dd8b79f5e69a5ac0f81334e43554cd801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
