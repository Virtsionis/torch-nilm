{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9243163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " base_models.py      __init__.py\t\t    Untitled.ipynb\r\n",
      " bayesian.py\t     models.py\t\t\t    vae_nilm.py\r\n",
      " bert.py\t     __pycache__\t\t    vae.py\r\n",
      " custom_modules.py  ' Pytorch input shapes.ipynb'   variational.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7c99a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "# from variational import VIBNet\n",
    "from custom_modules import ConvDropRelu, \\\n",
    "    LinearDropRelu, ConvBatchRelu, VIBDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502a2a5d",
   "metadata": {},
   "source": [
    "## Deconvolution Explained\n",
    "\n",
    "The transpose convolution operation has been used in many models where upsampling is needed. It is very similar to the convolution operation, only that the convolution matrix is transposed. Therefore the result is that the output grows instead of reducing(depending on the padding, stride and kernel size used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d97e0d3",
   "metadata": {},
   "source": [
    "**4d tensors must be in shape: [batch size, channels, height, width]**\n",
    "\n",
    "**Convolution**\n",
    "\n",
    "output = 1+ [(n+2p-f)/s]\n",
    "\n",
    "- n = is the input height(equals to input width) dimension\n",
    "- p = is the padding applied to the tensor\n",
    "- f = is the size of the square kernel\n",
    "- s = is the stride that will be applied with the convolution operation.\n",
    "- c = is the channel dimension of the tensor\n",
    "\n",
    "\n",
    "**DeConvolution** -> we solve the equation with respects to n and then we rename n->output, output->n\n",
    "\n",
    "output = s(n-1)+f-2p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eebec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b039b832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 16, 1])\n",
      "torch.Size([1, 512, 33, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.autograd import Variable \n",
    "random_tensor = Variable(torch.randn(1, 256, 16, 1)) \n",
    "upsample_tnsr = nn.ConvTranspose2d(in_channels = 256, out_channels = 512, \n",
    "                                   kernel_size = 3, stride = 2, padding = 0)(random_tensor)\n",
    "print(random_tensor.shape)\n",
    "print(upsample_tnsr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5fd96e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=2):\n",
    "        super(DeConv1d, self).__init__()\n",
    "\n",
    "        left, right = kernel_size * 2, kernel_size * 2\n",
    "        print(kernel_size)\n",
    "#         if kernel_size % 2 == 0:\n",
    "#             right -= kernel_size % 2\n",
    "#         padding = (left, right, 0, 0)\n",
    "\n",
    "        self.deconv = nn.Sequential(\n",
    "#             nn.ZeroPad2d(padding),\n",
    "            nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride),\n",
    "\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.deconv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "16812b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([1, 256, 16])\n",
      "torch.Size([1, 512, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.autograd import Variable \n",
    "random_tensor = Variable(torch.randn(1, 256, 16)) \n",
    "upsample_tnsr = nn.ConvTranspose1d(in_channels = 256, out_channels = 512, \n",
    "                                   kernel_size = 3, stride = 2, padding = 0)(random_tensor)\n",
    "upsample_tnsr = DeConv1d(in_channels = 256, out_channels = 512, \n",
    "                         kernel_size = 2, stride = 2)(random_tensor)\n",
    "print(random_tensor.shape)\n",
    "print(upsample_tnsr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1b03bfc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 256, 1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.randn(50, 256, 1)) \n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3af0972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 in x.shape:\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "20e1320a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 spatial element when training, got input size torch.Size([50, 256, 1])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-2c211eff12f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInstanceNorm1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/torch-nilm/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch-nilm/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_input_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         return F.instance_norm(\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             self.training or not self.track_running_stats, self.momentum, self.eps)\n",
      "\u001b[0;32m~/anaconda3/envs/torch-nilm/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36minstance_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, use_input_stats, momentum, eps)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         )\n\u001b[1;32m   2324\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_input_stats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m         \u001b[0m_verify_spatial_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2326\u001b[0m     return torch.instance_norm(\n\u001b[1;32m   2327\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_input_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch-nilm/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_verify_spatial_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2290\u001b[0m         \u001b[0msize_prods\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2291\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_prods\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2292\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected more than 1 spatial element when training, got input size {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected more than 1 spatial element when training, got input size torch.Size([50, 256, 1])"
     ]
    }
   ],
   "source": [
    "nn.InstanceNorm1d(256)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519426d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
