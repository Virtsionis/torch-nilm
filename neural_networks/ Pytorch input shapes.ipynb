{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89866f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import Trainer\n",
    "from torchnlp.nn.attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d9f5409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from blitz.modules import BayesianLinear\n",
    "from blitz.modules.conv_bayesian_layer import BayesianConv1d\n",
    "from blitz.modules.gru_bayesian_layer import BayesianGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1209a5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3af178d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        self.pe = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        return self.pe.weight.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(features))\n",
    "        self.bias = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.weight * (x - mean) / (std + self.eps) + self.bias\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "\n",
    "        x, attn = self.attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.output_linear(x)\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.activation = GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.activation(self.w_1(x)))\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.layer_norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return self.layer_norm(x + self.dropout(sublayer(x)))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(\n",
    "            h=attn_heads, d_model=hidden, dropout=dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(\n",
    "            d_model=hidden, d_ff=feed_forward_hidden)\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(\n",
    "            x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class BERT4NILM(nn.Module):\n",
    "    # def __init__(self, args):\n",
    "    def __init__(self, window_size,drop_out=0.5,output_size=1):\n",
    "        super().__init__()\n",
    "        # self.args = args\n",
    "        # self.dropout_rate = args.drop_out\n",
    "        # self.original_len = args.window_size\n",
    "        # self.output_size = args.output_size\n",
    "        self.original_len = window_size\n",
    "        self.latent_len = int(self.original_len / 2)\n",
    "        self.dropout_rate = drop_out\n",
    "\n",
    "        self.hidden = 256\n",
    "        self.heads = 2\n",
    "        self.n_layers = 2\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.conv = nn.Conv1d(in_channels=1, out_channels=self.hidden,\n",
    "                               kernel_size=5, stride=1, padding=2, padding_mode='replicate')\n",
    "        self.pool = nn.LPPool1d(norm_type=2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.position = PositionalEmbedding(\n",
    "            max_len=self.latent_len, d_model=self.hidden)\n",
    "        self.layer_norm = LayerNorm(self.hidden)\n",
    "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([TransformerBlock(\n",
    "            self.hidden, self.heads, self.hidden * 4, self.dropout_rate) for _ in range(self.n_layers)])\n",
    "\n",
    "        self.deconv = nn.ConvTranspose1d(\n",
    "            in_channels=self.hidden, out_channels=self.hidden, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.hidden, 128)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.linear2 = nn.Linear(128*window_size, 128)\n",
    "        self.out = nn.Linear(128, 1)     \n",
    "        self.truncated_normal_init()\n",
    "        \n",
    "\n",
    "    def truncated_normal_init(self, mean=0, std=0.02, lower=-0.04, upper=0.04):\n",
    "        params = list(self.named_parameters())\n",
    "        for n, p in params:\n",
    "            if 'layer_norm' in n:\n",
    "                continue\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    l = (1. + math.erf(((lower - mean) / std) / math.sqrt(2.))) / 2.\n",
    "                    u = (1. + math.erf(((upper - mean) / std) / math.sqrt(2.))) / 2.\n",
    "                    p.uniform_(2 * l - 1, 2 * u - 1)\n",
    "                    p.erfinv_()\n",
    "                    p.mul_(std * math.sqrt(2.))\n",
    "                    p.add_(mean)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        print(sequence.shape)\n",
    "        x_token = self.pool(self.conv(sequence.unsqueeze(1))).permute(0, 2, 1)\n",
    "        embedding = x_token + self.position(sequence)\n",
    "        x = self.dropout(self.layer_norm(embedding))\n",
    "        print(x.shape)\n",
    "\n",
    "        mask = None\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(x, mask)\n",
    "        print(x.shape)\n",
    "        x = self.deconv(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        print(x.shape)\n",
    "        \n",
    "        x = torch.tanh(self.linear1(x))\n",
    "        print(x.shape)\n",
    "        x = self.flat(x)\n",
    "        print(x.shape)\n",
    "        x = self.linear2(x)\n",
    "        print(x.shape)\n",
    "        out = self.out(x)\n",
    "        print(out.shape)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a29bb205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 50])\n",
      "torch.Size([1024, 25, 256])\n",
      "torch.Size([1024, 25, 256])\n",
      "torch.Size([1024, 50, 256])\n",
      "torch.Size([1024, 50, 128])\n",
      "torch.Size([1024, 6400])\n",
      "torch.Size([1024, 128])\n",
      "torch.Size([1024, 1])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "window = 50 #aka seq length\n",
    "# embedding_dim = 1\n",
    "x = torch.randn(batch_size, window) \n",
    "cf = BERT4NILM(window)\n",
    "x = x\n",
    "out = cf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9891b45f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cf96d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706f4ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9945711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e227c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FReal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2)\n",
    "        x = torch.fft.fft(x, dim=-1)\n",
    "        return x.real\n",
    "\n",
    "class FImag(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2)\n",
    "        x = torch.fft.fft(x, dim=-1)\n",
    "        return x.imag\n",
    "\n",
    "class _Cnn1(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dropout):\n",
    "        super(_Cnn1, self).__init__()\n",
    "\n",
    "        left, right = kernel_size//2, kernel_size//2\n",
    "        if kernel_size%2==0 :\n",
    "            right -= 1\n",
    "        padding = (left, right, 0, 0)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ZeroPad2d(padding),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "class ConvFourier(nn.Module):\n",
    "\n",
    "    def __init__(self, window_size, dropout=0, lr=None):\n",
    "        super(ConvFourier, self).__init__()\n",
    "        self.MODEL_NAME = 'ConvFourier'\n",
    "        self.drop = dropout\n",
    "        self.lr = lr\n",
    "        cnn_out = 8 #the out_features of last CNN\n",
    "        self.dense_input = cnn_out*window_size\n",
    "\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            _Cnn1(1, cnn_out, kernel_size=5, dropout=self.drop),\n",
    "            # nn.LPPool1d(norm_type=2, kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.freal = FReal()\n",
    "        self.fimag = FImag()\n",
    "        self.attention = Attention(window_size,attention_type='dot')\n",
    "        self.flat = nn.Flatten()\n",
    "        self.mlp = nn.Linear(self.dense_input, 1)\n",
    "#         self.mlp = nn.Sequential(\n",
    "#             nn.Linear(self.dense_input, 4*self.dense_input),\n",
    "#             nn.Dropout(self.drop),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(4*self.dense_input, self.dense_input),\n",
    "#             nn.Dropout(self.drop),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(self.dense_input, 1),\n",
    "#         )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x\n",
    "        x = x.unsqueeze(1)\n",
    "        cnn = self.conv(x)\n",
    "        print(cnn.shape)\n",
    "        real = self.freal(cnn)\n",
    "        imag = self.fimag(cnn)\n",
    "        print(imag.shape,real.shape)\n",
    "        attn, _ = self.attention(real,imag)\n",
    "        attn = self.flat(attn)\n",
    "        mlp = self.mlp(attn)\n",
    "        print(mlp.shape)\n",
    "        return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "68de7c27",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-fbd62558a639>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# embedding_dim = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvFourier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-07c207d8b280>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, window_size, dropout, lr)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFReal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfimag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFImag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m#         self.mlp = nn.Linear(self.dense_input, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "window = 50 #aka seq length\n",
    "# embedding_dim = 1\n",
    "x = torch.randn(batch_size, window) \n",
    "cf = ConvFourier(window)\n",
    "x = x\n",
    "out = cf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ce296646",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Dense(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout):\n",
    "        super(_Dense, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "      \n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FNetBlock(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2).real\n",
    "    return x\n",
    "\n",
    "class FNet(nn.Module):\n",
    "    def __init__(self, dim, depth, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, FNetBlock()),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "    \n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self, window, features, hidden_dim, depth, drop):\n",
    "        super(MyNet, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(1, features),\n",
    "#             nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "        \n",
    "        self.fnet = FNet(features, depth, hidden_dim, drop)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(features*window, 64)\n",
    "        self.dense2 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x\n",
    "        x = x.unsqueeze(2)\n",
    "        print(x.shape)\n",
    "        x = self.linear(x)\n",
    "        print(x.shape)\n",
    "        x = self.fnet(x)\n",
    "        print(x.shape)\n",
    "        x = self.flat(x)\n",
    "        print('flat', x.shape)\n",
    "        x = self.dense1(x)\n",
    "        print(x.shape)\n",
    "        x = self.dense2(x)\n",
    "        print(x.shape)\n",
    "        out = self.output(x)\n",
    "        print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2d09f6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 50, 1])\n",
      "torch.Size([1024, 50, 128])\n",
      "torch.Size([1024, 50, 128])\n",
      "flat torch.Size([1024, 6400])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 32])\n",
      "torch.Size([1024, 1])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "window = 50 #aka seq length\n",
    "# embedding_dim = 1\n",
    "x = torch.randn(batch_size, window) \n",
    "cf = MyNet(window, 128, 256, 1, 0.5)\n",
    "x = x\n",
    "out = cf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d04d3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 50, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 50, 16])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "window = 50 #aka seq length\n",
    "# embedding_dim = 1\n",
    "x = torch.randn(batch_size,window) \n",
    "\n",
    "x = x.unsqueeze(2)\n",
    "print(x.shape)\n",
    "linear_net = nn.Sequential(\n",
    "            nn.Linear(1, 16),)\n",
    "linear_net(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28a4f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "833cb5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Dense(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout):\n",
    "        super(_Dense, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class _Cnn1(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dropout):\n",
    "        super(_Cnn1, self).__init__()\n",
    "\n",
    "        left, right = kernel_size//2, kernel_size//2\n",
    "        if kernel_size%2==0 :\n",
    "            right -= 1\n",
    "        padding = (left, right, 0, 0)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ZeroPad2d(padding),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class FNETBLock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, dim_feedforward, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim - Dimensionality of the input\n",
    "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
    "            dropout - Dropout probability to use in the dropout layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Two-layer MLP\n",
    "        self.linear_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim_feedforward, input_dim)\n",
    "        )\n",
    "\n",
    "        # Layers to apply in between the main layers\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x\n",
    "        fft_out = torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2).real\n",
    "        x = x + self.dropout(fft_out)\n",
    "        print('shape dropout', x.shape)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.norm1(x)\n",
    "#         x = x.permute(0, 2, 1)\n",
    "        print('shape after norm', x.shape)\n",
    "\n",
    "        # MLP part\n",
    "        linear_out = self.linear_net(x)\n",
    "        print('shape after linear', x.shape)\n",
    "        x = x + self.dropout(linear_out)\n",
    "        x = self.norm2(x)\n",
    "        print('shape after norm2', x.shape)\n",
    "        return x\n",
    "\n",
    "class FNET(nn.Module):\n",
    "\n",
    "    def __init__(self, depth, kernel_size, cnn_dim, **block_args):\n",
    "        super(FNET, self).__init__()\n",
    "\n",
    "        drop = block_args['dropout']\n",
    "        input_dim = block_args['input_dim']\n",
    "        dense_in = input_dim*cnn_dim//2\n",
    "\n",
    "        self.conv = _Cnn1(1, cnn_dim, kernel_size=kernel_size,dropout=drop)\n",
    "        self.pool = nn.LPPool1d(norm_type=2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.fnet_layers = nn.ModuleList([FNETBLock(**block_args) for _ in range(depth)])\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "        self.dense1 = _Dense(dense_in, cnn_dim, drop)\n",
    "        self.dense2 = _Dense(cnn_dim, cnn_dim//2, drop)\n",
    "        self.output = nn.Linear(cnn_dim//2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x must be in shape [batch_size, 1, window_size]\n",
    "        # eg: [1024, 1, 50]\n",
    "        x = x\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        print('shape after cnn', x.shape)\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        x = self.pool(x)\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        print('shape after pool', x.shape)\n",
    "        for layer in self.fnet_layers:\n",
    "            x = layer(x)\n",
    "        print('shape after fblock', x.shape)\n",
    "        x = self.flat(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        out = self.output(x)\n",
    "        print(out.shape)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class FAED(nn.Module):\n",
    "\n",
    "    def __init__(self, depth, kernel_size, **block_args):\n",
    "        super(FAED, self).__init__()\n",
    "\n",
    "        drop = block_args['dropout']\n",
    "        dim_cnn = block_args['input_dim']\n",
    "        dim_feedforward = block_args['dim_feedforward']\n",
    "\n",
    "        self.conv = _Cnn1(1, dim_cnn, kernel_size=kernel_size,dropout=drop)\n",
    "        self.fnet_layers = nn.ModuleList([FNETBLock(**block_args) for _ in range(depth)])\n",
    "\n",
    "        self.dense = _Dense(dim_cnn, dim_feedforward*2, drop)\n",
    "        self.output = nn.Linear(dim_feedforward*2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x must be in shape [batch_size, 1, window_size]\n",
    "        # eg: [1024, 1, 50]\n",
    "        x = x\n",
    "        print(x.shape)\n",
    "        x = x.unsqueeze(1)\n",
    "        print(x.shape)\n",
    "        x = self.conv(x)\n",
    "        print(x.shape)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        for layer in self.fnet_layers:\n",
    "            x = layer(x)\n",
    "            print(x.shape)\n",
    "\n",
    "        x = self.dense(x)\n",
    "        print(x.shape)\n",
    "        out = self.output(x)\n",
    "        print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f596afe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7f6200bd595f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# cf = FNETBLock(50, 50*4, 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAED\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "window = 50 #aka seq length\n",
    "embedding_dim = 16\n",
    "x = torch.randn(batch_size, window, embedding_dim) \n",
    "# cf = FNETBLock(50, 50*4, 0)\n",
    "cf = FAED(50, 50*4, 0)\n",
    "x = x\n",
    "out = cf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96605593",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "window = 50 #aka seq length\n",
    "embedding_dim = 16\n",
    "x = torch.randn(batch_size, window, embedding_dim) \n",
    "print(x.shape)\n",
    "# x = x.permute(0, 2, 1)\n",
    "linear_net = nn.Linear(16, 200)\n",
    "# linear_net = nn.Sequential(\n",
    "#             nn.Linear(50, 200),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(200, 50)\n",
    "# )\n",
    "x = linear_net(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf17d427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38626a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3424a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "window = 50 #aka seq length\n",
    "embedding_dim = 1\n",
    "x = torch.randn(batch_size, window, embedding_dim) \n",
    "print(x.shape)\n",
    "x = x.transpose(1, 2).contiguous()\n",
    "print('input shape: ', x.shape)\n",
    "\n",
    "conv = _Cnn1(in_channels=1, out_channels=16, kernel_size=kernel_size,dropout=drop)\n",
    "x = conv(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce32734",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "window = 50 #aka seq length\n",
    "embedding_dim = 111\n",
    "x = torch.randn(batch_size, window, embedding_dim) \n",
    "# print(x.shape)\n",
    "x = x.transpose(1, 2).contiguous()\n",
    "print('input shape: ', x.shape)\n",
    "dim_cnn = 256\n",
    "input_dim = 50 #seq len\n",
    "hidden_dim = 512\n",
    "drop = 0.5\n",
    "linear_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Dropout(drop),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "# linear_net(x).shape\n",
    "x = _Dense(50, 125, .5)(x)\n",
    "print(x.shape)\n",
    "x = _Dense(125, 50, .5)(x)\n",
    "print(x.shape)\n",
    "x = _Dense(50, 1, .5)(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3520e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FReal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.fft.fft(x, dim=-1)\n",
    "        x = torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2)\n",
    "        return x.real\n",
    "\n",
    "class FImag(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2)\n",
    "        return x.imag\n",
    "class ConvFourier(nn.Module):\n",
    "\n",
    "    def __init__(self, window_size, dropout=0, lr=None):\n",
    "        super(ConvFourier, self).__init__()\n",
    "        self.MODEL_NAME = 'CNN model with fourier between linear layers'\n",
    "        self.drop = dropout\n",
    "        self.lr = lr\n",
    "\n",
    "        self.dense_input = 50*window_size #50 is the out_features of last CNNF\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            _Cnn1(1, 30, kernel_size=10, dropout=self.drop),\n",
    "            _Cnn1(30, 40, kernel_size=8, dropout=self.drop),\n",
    "            _Cnn1(40, 50, kernel_size=5, dropout=self.drop),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        # self.dense = _Dense(self.dense_input, 1024, self.drop)\n",
    "        self.linear = nn.Linear(self.dense_input, 2*self.dense_input)\n",
    "        self.freal = FReal()\n",
    "        self.fimag = FImag()\n",
    "        self.output = nn.Linear(4*self.dense_input, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x must be in shape [batch_size, 1, window_size]\n",
    "        # eg: [1024, 1, 50]\n",
    "        x = x\n",
    "        print('input', x.shape)\n",
    "\n",
    "        x = self.conv(x)\n",
    "        print('x after cnn', x.shape)\n",
    "        x = self.linear(x)\n",
    "        print('x after linear', x.shape)\n",
    "        x_r = self.freal(x) + x\n",
    "        x_im = self.fimag(x) + x\n",
    "        y = torch.cat([x_r, x_im], dim= -1)\n",
    "#         print('cat real+imag', y.shape)\n",
    "#         x = torch.cat([x,y], dim= -1)\n",
    "#         print('cat x+y', x.shape)\n",
    "        out = self.output(y)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f765dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "window = 50 #aka seq length\n",
    "embedding_dim = 1\n",
    "x = torch.randn(batch_size, window, embedding_dim) \n",
    "# print(x.shape)\n",
    "x = x.transpose(1, 2).contiguous()\n",
    "print('input shape: ', x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d16ea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = 0.1\n",
    "dense_input = 50*window\n",
    "cf = ConvFourier( window, drop)\n",
    "x = x\n",
    "out = cf(x)\n",
    "# conv = nn.Sequential(\n",
    "#             _Cnn1(1, 30, kernel_size=10, dropout=drop),\n",
    "#             _Cnn1(30, 40, kernel_size=8, dropout=drop),\n",
    "#             _Cnn1(50, 50, kernel_size=5, dropout=drop),\n",
    "#             nn.Flatten())\n",
    "# linear = nn.Linear(dense_input, 4*dense_input)\n",
    "# freal = FReal()\n",
    "# fimag = FImag()\n",
    "# output = nn.Linear(4*dense_input, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d58162",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3)\n",
    "print(x)\n",
    "torch.cat([x,x], dim= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176c51d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfdd4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880fe04a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce6f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FReal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2)\n",
    "#         x = torch.fft.fft(x, dim=-1)\n",
    "        return x.real\n",
    "\n",
    "class FImag(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2)\n",
    "#         x = torch.fft.fft(x, dim=-1)\n",
    "        return x.imag\n",
    "class ConvFourier2(nn.Module):\n",
    "\n",
    "    def __init__(self, window_size, dropout=0, lr=None):\n",
    "        super(ConvFourier2, self).__init__()\n",
    "        self.MODEL_NAME = 'CNN model with fourier between linear layers'\n",
    "        self.drop = dropout\n",
    "        self.lr = lr\n",
    "        cnn_out = 16 #the out_features of last CNN\n",
    "        self.dense_input = cnn_out*window_size\n",
    "\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            _Cnn1(1, cnn_out, kernel_size=11, dropout=self.drop),\n",
    "            # _Cnn1(30, 40, kernel_size=8, dropout=self.drop),\n",
    "            # _Cnn1(40, 50, kernel_size=6, dropout=self.drop),\n",
    "            # _Cnn1(50, 50, kernel_size=5, dropout=self.drop),\n",
    "            # _Cnn1(50, 50, kernel_size=5, dropout=self.drop),\n",
    "            # nn.Flatten()\n",
    "        )\n",
    "        self.freal = FReal()\n",
    "        self.fimag = FImag()\n",
    "\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(self.dense_input, 2*self.dense_input),\n",
    "            nn.Dropout(self.drop),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2*self.dense_input, self.dense_input)\n",
    "        )\n",
    "\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(self.dense_input, 2*self.dense_input),\n",
    "            nn.Dropout(self.drop),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2*self.dense_input, self.dense_input)\n",
    "        )\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "        self.dense1 = _Dense(2*self.dense_input, 1024, dropout=self.drop)\n",
    "        self.output = nn.Linear(1024, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x must be in shape [batch_size, 1, window_size]\n",
    "        # eg: [1024, 1, 50]\n",
    "        x = x\n",
    "#         x = x.unsqueeze(1)\n",
    "        cnn = self.conv(x)\n",
    "        print(cnn.shape)\n",
    "        real_x = self.freal(cnn)\n",
    "        imag_x = self.fimag(cnn)\n",
    "        real_x = self.flat(real_x)\n",
    "        imag_x = self.flat(imag_x)\n",
    "        cnn = self.flat(cnn)\n",
    "        mlp1 = self.mlp1(real_x) + cnn\n",
    "        mlp2 = self.mlp2(imag_x) + cnn\n",
    "        x = torch.cat([real_x, imag_x], dim= -1)\n",
    "        x = self.flat(x)\n",
    "        x = self.dense1(x)\n",
    "\n",
    "        out = self.output(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf75a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = ConvFourier2( window, drop)\n",
    "x = x\n",
    "out = cf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ada3266",
   "metadata": {},
   "source": [
    "# THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e359414",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "window = 50 #aka seq length\n",
    "embedding_dim = 1\n",
    "x = torch.randn(batch_size, window, embedding_dim) \n",
    "# print(x.shape)\n",
    "x = x.transpose(1, 2).contiguous()\n",
    "print('input shape: ', x.shape)\n",
    "\n",
    "dim_cnn = 128\n",
    "input_dim = 50 #seq len\n",
    "hidden_dim = 2*input_dim\n",
    "drop = 0.5\n",
    "ccc = _Cnn1(in_channels=1, out_channels=dim_cnn, kernel_size=5,dropout=drop)\n",
    "\n",
    "pool = nn.LPPool1d(norm_type=2, kernel_size=2, stride=2)\n",
    "m2pool = nn.MaxPool2d(3, stride=2)\n",
    "f = nn.Flatten()\n",
    "\n",
    "norm1 = nn.LayerNorm(input_dim)\n",
    "norm2 = nn.LayerNorm(input_dim)\n",
    "\n",
    "linear_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Dropout(drop),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "# linear_out = nn.Sequential(\n",
    "#             nn.Linear(50*128, 256),\n",
    "#             nn.Dropout(drop),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#         )\n",
    "d1 = _Dense(50*dim_cnn//2, dim_cnn,drop)\n",
    "d2 = _Dense(dim_cnn, dim_cnn//2, drop)\n",
    "output = nn.Linear(dim_cnn//2, 1)\n",
    "x = ccc(x)\n",
    "print('ccc out shape: ', x.shape)\n",
    "x = x.transpose(1, 2).contiguous()\n",
    "x = pool(x)\n",
    "# x = m2pool(x)\n",
    "x = x.transpose(1, 2).contiguous()\n",
    "print('pool out shape: ', x.shape)\n",
    "\n",
    "print('fft out shape: ', x.shape)\n",
    "\n",
    "x = x + torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2).real\n",
    "x = norm1(x)\n",
    "print('norm1 out shape: ', x.shape)\n",
    "\n",
    "\n",
    "x = x + linear_net(x)\n",
    "x = norm2(x)\n",
    "print('norm2 out shape: ', x.shape)\n",
    "x = f(x)\n",
    "print('flat out shape: ', x.shape)\n",
    "# x = linear_out(x)\n",
    "x = d1(x)\n",
    "print('d1 out shape: ', x.shape)\n",
    "x = d2(x)\n",
    "print('d2 out shape: ', x.shape)\n",
    "x = output(x)\n",
    "print('final out shape: ', x.shape)\n",
    "del x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac9b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "12//5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e4d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "128*50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool = nn.LPPool1d(norm_type=2, kernel_size=2, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc19dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "window = 50 #aka seq length\n",
    "embedding_dim = 1\n",
    "x = torch.randn(batch_size, window, embedding_dim) \n",
    "print(x.shape)\n",
    "x = x.transpose(1, 2).contiguous()\n",
    "print('input shape: ', x.shape)\n",
    "\n",
    "block_args = {'dropout':0.5,\n",
    "              'input_dim': 50,\n",
    "              'dim_feedforward': 64}\n",
    "\n",
    "depth = 1\n",
    "dim_cnn = 16\n",
    "kernel_size = 4\n",
    "drop = block_args['dropout']\n",
    "dim_cnn = block_args['input_dim']\n",
    "dim_feedforward = block_args['dim_feedforward']\n",
    "\n",
    "conv = _Cnn1(in_channels=1, out_channels=16, \n",
    "             kernel_size=kernel_size,dropout=drop)\n",
    "f = nn.Flatten()\n",
    "\n",
    "fnet_layers = nn.ModuleList([FNETBLock(input_dim=50, \n",
    "                                       dim_feedforward=64, dropout=0.0) for _ in range(depth)])\n",
    "\n",
    "dense = _Dense(dim_cnn, dim_feedforward*2, drop)\n",
    "output = nn.Linear(dim_feedforward*2, 1)\n",
    "\n",
    "\n",
    "x = x\n",
    "# x = x.unsqueeze(2)\n",
    "print(x.shape)\n",
    "x = conv(x)\n",
    "print('conv out shape:', x.shape)\n",
    "x = f(x)\n",
    "print('f out shape: ', x.shape)\n",
    "for layer in fnet_layers:\n",
    "    x = layer(x)\n",
    "    print('fblock out shape: ', x.shape)\n",
    "\n",
    "x = dense(x)\n",
    "print('dense out: ', x.shape)\n",
    "out = output(x)\n",
    "print('final out shape: ', x.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c735cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae7904d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14636e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80792977",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [10, 8, 6, 5]\n",
    "for kernel_size in kernels:\n",
    "    left, right = kernel_size//2, kernel_size//2\n",
    "    if kernel_size%2==0 :\n",
    "        print(kernel_size, 'is even number')\n",
    "        right -= 1\n",
    "    else:\n",
    "        print(kernel_size, 'is odd number')\n",
    "    padding = (left, right, 0, 0)\n",
    "    print(padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acacd1c2",
   "metadata": {},
   "source": [
    "# FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f42ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "window = 50 #aka seq length\n",
    "embedding_dim = 1\n",
    "x = torch.randn(batch_size, window, embedding_dim) \n",
    "print(x.shape)\n",
    "fft2 = torch.fft.fft2(x).real\n",
    "two_ffts = torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2).real\n",
    "# fft2.real\n",
    "# two_ffts = torch.fft.fft(torch.fft.fft(x, dim=0), dim=1)\n",
    "torch.allclose(fft2, two_ffts)\n",
    "two_ffts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de2f5d7",
   "metadata": {},
   "source": [
    "# Conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2541f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [batch_size, sequence_length, embedding_dim]\n",
    "\n",
    "# convolution_layer = nn.conv1d(in_channels, out_channels, kernel_size)\n",
    "# in_channels = embedding_dim\n",
    "# out_channels = arbitrary int\n",
    "# kernel_size = 2 (I want bigrams)\n",
    "batch_size = 1024\n",
    "window = 50 #aka seq length\n",
    "embedding_dim = 1\n",
    "x = torch.randn(batch_size, window, embedding_dim) \n",
    "print(x.shape)\n",
    "x = x.transpose(1, 2).contiguous()\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=16, kernel_size=4,stride=1)\n",
    "feature_map = conv1(x)\n",
    "feature_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f28c4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(1024, 16, 47)\n",
    "# With Learnable Parameters\n",
    "# m = nn.LayerNorm(input.size()[1:])\n",
    "# # Without Learnable Parameters\n",
    "# m = nn.LayerNorm(input.size()[1:], elementwise_affine=False)\n",
    "# # Normalize over last two dimensions\n",
    "# m = nn.LayerNorm([10, 10])\n",
    "# Normalize over last dimension of size 16\n",
    "inp = inp.permute(0, 2, 1)\n",
    "print(inp.shape)\n",
    "m = nn.LayerNorm(16)\n",
    "# Activating the module\n",
    "output = m(inp)\n",
    "output = output.permute(0, 2, 1)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf18ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m = nn.LayerNorm(16)\n",
    "output = m(feature_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8275934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1024, 1, 10)  \n",
    "print(a.size())\n",
    "a = nn.ZeroPad2d((5,4,0,0))(a)\n",
    "a = nn.Conv1d(1, 30, kernel_size=10)(a)\n",
    "print(a.size())\n",
    "a = nn.ZeroPad2d((3,4,0,0))(a)\n",
    "a = nn.Conv1d(30, 40, kernel_size=8)(a)\n",
    "print(a.size())\n",
    "a = nn.ZeroPad2d((2,3,0,0))(a)\n",
    "a = nn.Conv1d(40, 50, kernel_size=6)(a)\n",
    "print(a.size())\n",
    "a = nn.ZeroPad2d((2,2,0,0))(a)\n",
    "a = nn.Conv1d(50, 50, kernel_size=5)(a)\n",
    "print(a.size())\n",
    "a = nn.ZeroPad2d((2,2,0,0))(a)\n",
    "a = nn.Conv1d(50, 50, kernel_size=5)(a)\n",
    "print(a.size())\n",
    "a = nn.Flatten()(a)\n",
    "print(a.size())\n",
    "in_features = a.size()[1]\n",
    "out_features = 1024\n",
    "a = nn.Linear(in_features, out_features)(a)\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102486ae",
   "metadata": {},
   "source": [
    "## ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b160a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple attention layer (from torchnlp) like keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6cea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = feature_map.permute(0, 1,2)\n",
    "print(x.shape)\n",
    "attention = Attention(47)\n",
    "out, weights = attention(x, x)\n",
    "out.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2483fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled attention / multihead attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8501e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 1024\n",
    "# window = 50 #aka seq length\n",
    "# embedding_dim = 1\n",
    "# x = torch.randn(batch_size, window, embedding_dim) \n",
    "x = feature_map\n",
    "x = x.permute(0, 2, 1)\n",
    "query, key, value =  x, x, x\n",
    "# multihead_attn = nn.MultiheadAttention(embed_dim=16, num_heads=16)\n",
    "# attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "attn_output, attn_output_weights = scaled_dot_product(query, key, value)\n",
    "print('weights shape: ', attn_output_weights.shape, 'attn_output shape: ', attn_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdb4d5c",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccfe4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = feature_map.permute(0, 2, 1)\n",
    "feature_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a9234",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e073ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input of shape (batch, seqlength ,input_size)\n",
    "# output of shape (batch, seqlength, hiddensize*2)\n",
    "b1 = nn.GRU(input_size=16, hidden_size=64, batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.5)\n",
    "b1_out = b1(feature_map)[0]\n",
    "b1_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048f27d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = nn.GRU(128, 256, batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.5)\n",
    "b2_out = b2(b1_out)[0]\n",
    "b2_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0759a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "b2_out = b2_out[:, -1, :]\n",
    "b2_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dee61d",
   "metadata": {},
   "source": [
    "# Dense/Linear/FC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6450e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Dense(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout):\n",
    "        super(_Dense, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f65e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = b2_out[:, -1, :]\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcacd16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1 = _Dense(512, 128, 0.5)\n",
    "dense_out1 = dense1(x)\n",
    "dense_out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d3abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense2 = _Dense(128, 64, 0.5)\n",
    "dense_out2 = dense2(dense_out1)\n",
    "dense_out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c7a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = nn.Linear(64, 1)\n",
    "out = output(dense_out2)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c4472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
